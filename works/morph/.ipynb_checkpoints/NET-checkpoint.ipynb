{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "protected-darwin",
   "metadata": {},
   "source": [
    "Морфологический анализатор на основе ансамбля сверточной и LSTM сетей!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8f4bd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from pymorphy2.tagset import OpencorporaTag\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "from navec import Navec\n",
    "from slovnet.model.emb import NavecEmbedding\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e61caca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nesemenpolkov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\slovnet\\model\\emb.py:46: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  torch.from_numpy(navec.pq.indexes),\n"
     ]
    }
   ],
   "source": [
    "navec = Navec.load(\"../SHEM/VECS/vectors.bin\")\n",
    "\n",
    "emb = NavecEmbedding(navec)\n",
    "\n",
    "m = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0e1fa8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3f86a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTS_OF_SPEECH = [\n",
    "    \"UNK\", \n",
    "    'PART',\n",
    "    'ADP',\n",
    "    'ADV',\n",
    "    'INTJ',\n",
    "    'ADJ',\n",
    "    'DET',\n",
    "    'PRON',\n",
    "    'SCONJ',\n",
    "    'PROPN',\n",
    "    'CCONJ',\n",
    "    'AUX',\n",
    "    'VERB',\n",
    "    'NOUN',\n",
    "    'PUNCT'\n",
    "]\n",
    "\n",
    "ANIMACY = [\n",
    "        'Unk',\n",
    "        'Anim',  # одушевлённое\n",
    "        'Inan',  # неодушевлённое\n",
    "    ]\n",
    "\n",
    "GENDERS = [\n",
    "        'Unk',\n",
    "        'Masc',  # мужской род\n",
    "        'Fem',  # женский род\n",
    "        'Neut',  # средний род\n",
    "    ]\n",
    "\n",
    "NUMBERS = [\n",
    "        'Unk',\n",
    "        'Sing',  # единственное число\n",
    "        'Plur',  # множественное число\n",
    "    ]\n",
    "\n",
    "CASES = [\n",
    "    'Unk',\n",
    "    'Acc',\n",
    "    'Nom',\n",
    "    'Gen',\n",
    "    'Dat',\n",
    "    'Loc',\n",
    "    'Ins',\n",
    "    'Voc'\n",
    "]\n",
    "\n",
    "TENSES = [\n",
    "        'Unk',\n",
    "        'Pres',  # настоящее время\n",
    "        'Past',  # прошедшее время\n",
    "        'Fut',  # будущее время\n",
    "    ]\n",
    "\n",
    "speech_part_map = {str(s): i for i, s in enumerate(PARTS_OF_SPEECH)}\n",
    "speech_part_len = len(PARTS_OF_SPEECH)\n",
    "\n",
    "case_map = {str(s): i for i, s in enumerate(CASES)}\n",
    "case_len = len(CASES)\n",
    "\n",
    "number_map = {str(s): i for i, s in enumerate(NUMBERS)}\n",
    "number_len = len(NUMBERS)\n",
    "\n",
    "gender_map = {str(s): i for i, s in enumerate(GENDERS)}\n",
    "gender_len = len(GENDERS)\n",
    "\n",
    "tense_map = {str(s): i for i, s in enumerate(TENSES)}\n",
    "tense_len = len(TENSES)\n",
    "\n",
    "animacy_map = {str(s): i for i, s in enumerate(ANIMACY)}\n",
    "animacy_len = len(ANIMACY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c2a7ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = speech_part_len + case_len + number_len + gender_len + tense_len + animacy_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b2b30f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze2tensor(analyzer_result):\n",
    "    word, info = analyzer_result[0][0], analyzer_result[0][1]\n",
    "    \n",
    "#     pos_array = [0 for i in range(speech_part_len)]\n",
    "#     pos_array[speech_part_map.get(info.POS, 0)] = 1\n",
    "    \n",
    "#     case_array = [0 for i in range(case_len)]\n",
    "#     case_array[case_map.get(info.case, 0)] = 1\n",
    "    \n",
    "#     number_array = [0 for i in range(number_len)]\n",
    "#     number_array[number_map.get(info.number, 0)] = 1\n",
    "    \n",
    "#     gender_array = [0 for i in range(gender_len)]\n",
    "#     gender_array[gender_map.get(info.gender, 0)] = 1\n",
    "    \n",
    "#     tense_array = [0 for i in range(tense_len)]\n",
    "#     tense_array[tense_map.get(info.tense, 0)] = 1\n",
    "    \n",
    "#     animacy_array = [0 for i in range(animacy_len)]\n",
    "#     animacy_array[animacy_map.get(info.animacy, 0)] = 1\n",
    "    \n",
    "    try:\n",
    "        word_index = navec.vocab[word]\n",
    "    except KeyError:\n",
    "        word_index = 500000\n",
    "    finally:\n",
    "        word_tensor = emb(torch.tensor([word_index])).squeeze(0)\n",
    "    \n",
    "#     return torch.cat((word_tensor, \n",
    "#                       torch.tensor(pos_array, dtype=torch.long), \n",
    "#                       torch.tensor(case_array, dtype=torch.long),\n",
    "#                       torch.tensor(number_array, dtype=torch.long),\n",
    "#                       torch.tensor(gender_array, dtype=torch.long),\n",
    "#                       torch.tensor(tense_array, dtype=torch.long),\n",
    "#                       torch.tensor(animacy_array, dtype=torch.long)\n",
    "#                      ))\n",
    "    return word_tensor\n",
    "\n",
    "def parse2tensor(parse_result):\n",
    "    word, info = parse_result[:1], parse_result[1:]\n",
    "    \n",
    "    pos_array = [0 for i in range(speech_part_len)]\n",
    "    pos_array[speech_part_map.get(info[0], 0)] = 1\n",
    "    \n",
    "    case_array = [0 for i in range(case_len)]\n",
    "    case_array[case_map.get(info[1], 0)] = 1\n",
    "    \n",
    "    number_array = [0 for i in range(number_len)]\n",
    "    number_array[number_map.get(info[2], 0)] = 1\n",
    "    \n",
    "    gender_array = [0 for i in range(gender_len)]\n",
    "    gender_array[gender_map.get(info[3], 0)] = 1\n",
    "    \n",
    "    tense_array = [0 for i in range(tense_len)]\n",
    "    tense_array[tense_map.get(info[4], 0)] = 1\n",
    "    \n",
    "    animacy_array = [0 for i in range(animacy_len)]\n",
    "    animacy_array[animacy_map.get(info[5], 0)] = 1\n",
    "    \n",
    "    try:\n",
    "        word_index = navec.vocab[word]\n",
    "    except KeyError:\n",
    "        word_index = 500000\n",
    "    finally:\n",
    "        word_tensor = emb(torch.tensor([word_index])).squeeze(0)\n",
    "    \n",
    "    return torch.cat((torch.tensor(pos_array, dtype=torch.long), \n",
    "            torch.tensor(case_array, dtype=torch.long),\n",
    "            torch.tensor(number_array, dtype=torch.long),\n",
    "            torch.tensor(gender_array, dtype=torch.long),\n",
    "            torch.tensor(tense_array, dtype=torch.long),\n",
    "            torch.tensor(animacy_array, dtype=torch.long)\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1fda9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(path):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    i = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            i += 1\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "            else:\n",
    "                splited = line.split(\"\\t\")\n",
    "                tags = splited[6].split(\"|\")\n",
    "                case = '_'\n",
    "                number = '_'\n",
    "                gender = '_'\n",
    "                tense = '_'\n",
    "                animacy = '_'\n",
    "                for tag in tags:\n",
    "                    if tag.startswith('Case='):\n",
    "                        tag = tag.split(\"=\")[1]\n",
    "                        case = tag\n",
    "                    elif tag.startswith('Number='):\n",
    "                        tag = tag.split(\"=\")[1]\n",
    "                        number = tag\n",
    "                    elif tag.startswith('Gender='):\n",
    "                        tag = tag.split(\"=\")[1]\n",
    "                        gender = tag\n",
    "                    elif tag.startswith('Tense='):\n",
    "                        tag = tag.split(\"=\")[1]\n",
    "                        tense = tag\n",
    "                    elif tag.startswith('Animacy='):\n",
    "                        tag = tag.split(\"=\")[1]\n",
    "                        animacy = tag\n",
    "                        \n",
    "                sentence.append((splited[1], splited[5], \n",
    "                                 case, number, \n",
    "                                 gender, tense, animacy))\n",
    "            \n",
    "    return sentences\n",
    "    \n",
    "def vectorize_prepared_data(data):\n",
    "    i = 0\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for sentence in tqdm(data, total=len(data)):\n",
    "        i += 1\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        for j in range(len(sentence) - 1):\n",
    "            analyzer_result = None\n",
    "            if sentence[j][0] and sentence[j][1] in PARTS_OF_SPEECH:\n",
    "                analyzer_result = m.parse(sentence[j][0])\n",
    "                item = sentence[j]\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "#             if sentence[j + 1][0] and sentence[j + 1][1] in PARTS_OF_SPEECH:\n",
    "#                 item = sentence[j + 1]\n",
    "#             else:\n",
    "#                 continue\n",
    "\n",
    "            inputs.append(analyze2tensor(analyzer_result))\n",
    "            targets.append(parse2tensor(item))\n",
    "        if not inputs or not targets:\n",
    "            continue\n",
    "        \n",
    "        x.append(inputs)\n",
    "        y.append(targets)\n",
    "        \n",
    "        if i % 100000 == 0 and i > 0:\n",
    "            print(\"Passed:\", i)\n",
    "    print(\"Total data:\", i)\n",
    "    return x, y\n",
    "\n",
    "class MorphDataset(Dataset):\n",
    "    def __init__(self, path, threshold=None):\n",
    "        data = prepare_data(path)\n",
    "        print(\"Data prepared!\")\n",
    "        print(\"*\" * 50)\n",
    "        inputs, targets = vectorize_prepared_data(data)\n",
    "        if threshold:\n",
    "            inputs = inputs[:threshold]\n",
    "            targets = targets[:threshold]\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.inputs[idx]\n",
    "        y = self.targets[idx]\n",
    "        x = torch.cat([t.unsqueeze(0) for t in x], dim=0).to(device)\n",
    "        y = torch.cat([t.unsqueeze(0) for t in y], dim=0).to(device)\n",
    "        return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e94d0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared!\n",
      "**************************************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d4d3d35adf41fc86e01a6567bc3e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 49469\n",
      "Data prepared!\n",
      "**************************************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d5cc686ea64e728908226938b6fef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12420 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 12420\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(MorphDataset(\"labeled_syntagrus.train\"), batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(MorphDataset(\"labeled_syntagrus.test\"), batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "937e5f4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12416"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i = 0\n",
    "# for batch in train_loader:\n",
    "#     print(batch[0].shape, batch[1].shape)\n",
    "#     for i in range(batch[0].size(1)):\n",
    "#         print(batch[0][:, i].shape, batch[1][:, i].shape)\n",
    "#     if i == 2:\n",
    "#         break\n",
    "#     i += 1\n",
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17f95581",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, kernel_size=8, window_size=4):\n",
    "        super().__init__()\n",
    "        self.gru = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.hidden = hidden_size\n",
    "        self.conv = nn.Sequential(\n",
    "                                nn.Conv1d(in_channels=1, out_channels=1, kernel_size=8),\n",
    "                                nn.GELU(),\n",
    "                                nn.MaxPool1d(kernel_size=4),\n",
    "                                nn.Conv1d(in_channels=1, out_channels=1, kernel_size=8),\n",
    "                                nn.GELU(),\n",
    "                                nn.MaxPool1d(kernel_size=4)\n",
    "                                )\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear((((hidden_size - 8 + 1) // 4) - 8 + 1) // 4, output_size),\n",
    "            nn.GELU()\n",
    "            )\n",
    "        self.pos = nn.Linear(speech_part_len, speech_part_len)\n",
    "        self.case = nn.Linear(case_len, case_len)\n",
    "        self.number = nn.Linear(number_len, number_len)\n",
    "        self.gender = nn.Linear(gender_len, gender_len)\n",
    "        self.tense = nn.Linear(tense_len, tense_len)\n",
    "        self.animacy = nn.Linear(animacy_len, animacy_len)\n",
    "        \n",
    "    def forward(self, hx, cx, x):\n",
    "        hx, cx = self.gru(x, (hx, cx))\n",
    "        y = self.linear(self.conv(hx))\n",
    "        pos = self.pos(y[:, :speech_part_len])\n",
    "        case = self.case(y[:, speech_part_len: speech_part_len + case_len])\n",
    "        number = self.number(y[:, speech_part_len + case_len: speech_part_len + case_len + number_len])\n",
    "        gender = self.gender(y[:, speech_part_len + case_len + number_len: speech_part_len + case_len + number_len + gender_len])\n",
    "        tense = self.tense(y[:, speech_part_len + case_len + number_len + gender_len: speech_part_len + case_len + number_len + gender_len + tense_len])\n",
    "        animacy = self.animacy(y[:, speech_part_len + case_len + number_len + gender_len + tense_len: ])\n",
    "        return hx, cx, pos, case, number, gender, tense, animacy\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden).to(device), torch.zeros(1, self.hidden).to(device)\n",
    "        \n",
    "        \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, kernel_size=8, window_size=4):\n",
    "        super().__init__()\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.i2o = nn.Linear(hidden_size, hidden_size)\n",
    "        self.hidden = hidden_size\n",
    "        self.conv = nn.Sequential(\n",
    "                                nn.Conv1d(in_channels=1, out_channels=1, kernel_size=8),\n",
    "                                nn.GELU(),\n",
    "                                nn.MaxPool1d(kernel_size=4),\n",
    "                                nn.Conv1d(in_channels=1, out_channels=1, kernel_size=8),\n",
    "                                nn.GELU(),\n",
    "                                nn.MaxPool1d(kernel_size=4)\n",
    "                                    )\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.pos = nn.Linear(speech_part_len, speech_part_len)\n",
    "        self.case = nn.Linear(case_len, case_len)\n",
    "        self.number = nn.Linear(number_len, number_len)\n",
    "        self.gender = nn.Linear(gender_len, gender_len)\n",
    "        self.tense = nn.Linear(tense_len, tense_len)\n",
    "        self.animacy = nn.Linear(animacy_len, animacy_len)\n",
    "    \n",
    "    def forward(self, h_prev, x):\n",
    "        combined = torch.cat([h_prev, x], dim = 1) # concatenate x and h\n",
    "        h = torch.tanh(self.dropout(self.i2h(combined)))\n",
    "        y = self.i2o(h)\n",
    "        y = self.conv(y)\n",
    "        pos = self.softmax(self.pos(y[:, :speech_part_len]))\n",
    "        case = self.softmax(self.case(y[:, speech_part_len: speech_part_len + case_len]))\n",
    "        number = self.softmax(self.number(y[:, speech_part_len + case_len: speech_part_len + case_len + number_len]))\n",
    "        gender = self.softmax(self.gender(y[:, speech_part_len + case_len + number_len: speech_part_len + case_len + number_len + gender_len]))\n",
    "        tense = self.softmax(self.tense(y[:, speech_part_len + case_len + number_len + gender_len: speech_part_len + case_len + number_len + gender_len + tense_len]))\n",
    "        animacy = self.softmax(self.animacy(y[:, speech_part_len + case_len + number_len + gender_len + tense_len: ]))\n",
    "        return h, pos, case, number, gender, tense, animacy\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3136e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = speech_part_len\n",
    "c = case_len\n",
    "n = number_len\n",
    "g = gender_len\n",
    "t = tense_len\n",
    "a = animacy_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "890f87fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, y):\n",
    "    correct = (torch.softmax(pred, dim=1).argmax(-1) == y.argmax(-1)).sum()\n",
    "    return correct / y.size(0)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    seq_len = len(iterator)\n",
    "    lp, lc, ln, lg, lt, la = [], [], [], [], [], []\n",
    "    ap, ac, an, ag, at, aa = [], [], [], [], [], []\n",
    "    epoch_pos_acc = 0\n",
    "    epoch_case_acc = 0\n",
    "    epoch_number_acc = 0\n",
    "    epoch_gender_acc = 0\n",
    "    epoch_tense_acc = 0\n",
    "    epoch_animacy_acc = 0\n",
    "    \n",
    "    epoch_pos_loss = 0\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        i += 1\n",
    "        \n",
    "        loss = 0\n",
    "        loss_pos = 0\n",
    "        loss_case = 0\n",
    "        loss_number = 0\n",
    "        loss_gender = 0\n",
    "        loss_tense = 0\n",
    "        loss_animacy = 0\n",
    "        \n",
    "        pos_acc = 0\n",
    "        case_acc = 0\n",
    "        number_acc = 0\n",
    "        gender_acc = 0\n",
    "        tense_acc = 0\n",
    "        animacy_acc = 0\n",
    "        \n",
    "        hx, cx = model.initHidden()\n",
    "        inp, out = batch\n",
    "        for j in range(inp.size(1)):\n",
    "            hx, cx, pos, case, number, gender, tense, animacy = model(hx, cx, inp[:, j])\n",
    "            loss_pos += criterion(pos, out[:, j, : p].float())\n",
    "            loss_case += criterion(case, out[:, j, p: p + c].float())\n",
    "            loss_number += criterion(number, out[:, j, p + c: p + c + n].float())\n",
    "            loss_gender += criterion(gender, out[:, j, p + c + n: p + c + n + g].float())\n",
    "            loss_tense += criterion(tense, out[:, j, p + c + n + g: p + c + n + g + t].float())\n",
    "            loss_animacy += criterion(animacy, out[:, j, p + c + n + g + t:].float())\n",
    "            \n",
    "            pos_acc += accuracy(pos, out[:, j, : p])\n",
    "            case_acc += accuracy(case, out[:, j, p: p + c])\n",
    "            number_acc += accuracy(number, out[:, j, p + c: p + c + n])\n",
    "            gender_acc += accuracy(gender, out[:, j, p + c + n: p + c + n + g])\n",
    "            tense_acc += accuracy(tense, out[:, j, p + c + n + g: p + c + n + g + t])\n",
    "            animacy_acc += accuracy(animacy, out[:, j, p + c + n + g + t:])\n",
    "        \n",
    "        lp.append(loss_pos.item())\n",
    "        lc.append(loss_case.item())\n",
    "        ln.append(loss_number.item())\n",
    "        lg.append(loss_gender.item())\n",
    "        lt.append(loss_tense.item())\n",
    "        la.append(loss_animacy.item())\n",
    "        \n",
    "        ap.append(pos_acc / inp.size(1))\n",
    "        ac.append(case_acc / inp.size(1))\n",
    "        an.append(number_acc / inp.size(1))\n",
    "        ag.append(gender_acc / inp.size(1))\n",
    "        at.append(tense_acc / inp.size(1))\n",
    "        aa.append(animacy_acc / inp.size(1))\n",
    "        \n",
    "        epoch_pos_acc += (pos_acc / inp.size(1))\n",
    "        epoch_case_acc += (case_acc / inp.size(1))\n",
    "        epoch_number_acc += (number_acc / inp.size(1))\n",
    "        epoch_gender_acc += (gender_acc / inp.size(1))\n",
    "        epoch_tense_acc += (tense_acc / inp.size(1))\n",
    "        epoch_animacy_acc += (animacy_acc / inp.size(1))\n",
    "        \n",
    "        epoch_pos_loss += loss_pos.item()\n",
    "        loss = loss_pos + loss_case + loss_number + loss_gender + loss_tense + loss_animacy\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            print(\"_\" * 90)\n",
    "            print(f\"| passed: {i}/{seq_len} | loss: {(epoch_loss / i):7.4f} | loss pos: {(epoch_pos_loss / i):7.4f} |\"\n",
    "                  f\" pos_acc: {(epoch_pos_acc / i):7.4f} | case_acc: {(epoch_case_acc / i):7.4f} | \"\n",
    "                  f\"number_acc: {(epoch_number_acc / i):7.4f} | tense_acc: {(epoch_tense_acc / i):7.4f} | \"\n",
    "                  f\"pos_acc per last sentence: {((pos_acc / inp.size(1))):7.4f}\")\n",
    "    return epoch_loss / seq_len, lp, lc, ln, lg, lt, la, ap, ac, an, ag, at, aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de59fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    seq_len = len(iterator)\n",
    "    \n",
    "    epoch_tense_loss = 0\n",
    "    epoch_gender_loss = 0\n",
    "    epoch_number_loss = 0\n",
    "    epoch_case_loss = 0\n",
    "    epoch_pos_loss = 0\n",
    "    epoch_loss = 0\n",
    "    i = 0\n",
    "    \n",
    "    acc = 0\n",
    "    epoch_pos_acc = 0\n",
    "    epoch_case_acc = 0\n",
    "    epoch_number_acc = 0\n",
    "    epoch_gender_acc = 0\n",
    "    epoch_tense_acc = 0\n",
    "    epoch_animacy_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            i += 1\n",
    "            \n",
    "            loss = 0\n",
    "            loss_pos = 0\n",
    "            loss_case = 0\n",
    "            loss_number = 0\n",
    "            loss_gender = 0\n",
    "            loss_tense = 0\n",
    "            loss_animacy = 0\n",
    "            \n",
    "            pos_acc = 0\n",
    "            case_acc = 0\n",
    "            number_acc = 0\n",
    "            gender_acc = 0\n",
    "            tense_acc = 0\n",
    "            animacy_acc = 0\n",
    "            \n",
    "            hx, cx = model.initHidden()\n",
    "            inp, out = batch\n",
    "            for j in range(inp.size(1)):\n",
    "                hx, cx, pos, case, number, gender, tense, animacy = model(hx, cx, inp[:, j])\n",
    "                loss_pos += criterion(pos, out[:, j, : p].float())\n",
    "                loss_case += criterion(case, out[:, j, p: p + c].float())\n",
    "                loss_number += criterion(number, out[:, j, p + c: p + c + n].float())\n",
    "                loss_gender += criterion(gender, out[:, j, p + c + n: p + c + n + g].float())\n",
    "                loss_tense += criterion(tense, out[:, j, p + c + n + g: p + c + n + g + t].float())\n",
    "                loss_animacy += criterion(animacy, out[:, j, p + c + n + g + t:].float())\n",
    "                \n",
    "                pos_acc += accuracy(pos, out[:, j, : p].float())\n",
    "                case_acc += accuracy(case, out[:, j, p: p + c].float())\n",
    "                number_acc += accuracy(number, out[:, j, p + c: p + c + n].float())\n",
    "                gender_acc += accuracy(gender, out[:, j, p + c + n: p + c + n + g].float())\n",
    "                tense_acc += accuracy(tense, out[:, j, p + c + n + g: p + c + n + g + t].float())\n",
    "                animacy_acc += accuracy(animacy, out[:, j, p + c + n + g + t:].float())\n",
    "            acc += ((pos_acc + case_acc + number_acc + gender_acc + tense_acc + animacy_acc) / (6 * inp.size(1)))      \n",
    "            loss += (loss_pos + loss_case + loss_number + loss_gender + loss_tense + loss_animacy)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            epoch_pos_acc += (pos_acc / inp.size(1))\n",
    "            epoch_case_acc += (case_acc / inp.size(1))\n",
    "            \n",
    "            epoch_pos_loss += loss_pos.item()\n",
    "            epoch_tense_loss += loss_tense.item()\n",
    "            epoch_gender_loss += loss_gender.item()\n",
    "            epoch_number_loss += loss_number.item()\n",
    "            epoch_case_loss += loss_case.item()\n",
    "            \n",
    "            if i % 2000 == 0:\n",
    "                print(\"_\" * 100)\n",
    "                print(f\"| passed: {i}/{seq_len} | loss: {(epoch_loss / i):7.4f} | loss pos: {(epoch_pos_loss / i):7.4f} | \"\n",
    "                      f\"loss case: {(epoch_case_loss / i):7.4f} | loss number: {(epoch_number_loss / i):7.4f} | \"\n",
    "                      f\"loss gender: {(epoch_gender_loss / i):7.4f} | loss tense: {(epoch_tense_loss / i):7.4f} | \"\n",
    "                      f\"pos accuracy: {(pos_acc / inp.size(1)):6.3f} | case accuracy: {(case_acc / inp.size(1)):6.3f} |\")\n",
    "        \n",
    "    return epoch_loss / seq_len, acc / seq_len, epoch_pos_acc / seq_len, epoch_case_acc / seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7183f820",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = 300\n",
    "HIDDEN = 1024\n",
    "OUTPUT = out\n",
    "EPOCH = 4\n",
    "\n",
    "model = ContextLSTM(INPUT, HIDDEN, OUTPUT).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ab48b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters: 5433984\n"
     ]
    }
   ],
   "source": [
    "print(\"model parameters:\", sum([p.numel() for p in model.parameters() if p.requires_grad]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ae0c2c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________\n",
      "| passed: 10000/49449 | loss: 59.7522 | loss pos: 16.3279 | pos_acc:  0.6616 | case_acc:  0.7321 | number_acc:  0.8311 | tense_acc:  0.9071 | pos_acc per last sentence:  0.8667\n",
      "__________________________________________________________________________________________\n",
      "| passed: 20000/49449 | loss: 44.3961 | loss pos: 11.9552 | pos_acc:  0.7549 | case_acc:  0.8092 | number_acc:  0.8799 | tense_acc:  0.9351 | pos_acc per last sentence:  0.8667\n",
      "__________________________________________________________________________________________\n",
      "| passed: 30000/49449 | loss: 37.3520 | loss pos:  9.8948 | pos_acc:  0.7990 | case_acc:  0.8419 | number_acc:  0.9000 | tense_acc:  0.9473 | pos_acc per last sentence:  1.0000\n",
      "__________________________________________________________________________________________\n",
      "| passed: 40000/49449 | loss: 33.2877 | loss pos:  8.7093 | pos_acc:  0.8240 | case_acc:  0.8610 | number_acc:  0.9117 | tense_acc:  0.9540 | pos_acc per last sentence:  1.0000\n",
      "====================================================================================================\n",
      "| epoch: 1/4 | train loss: 30.56555567995454 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 2000/12416 | loss: 18.9596 | loss pos:  4.5724 | loss case:  4.0419 | loss number:  2.2949 | loss gender:  3.9233 | loss tense:  1.2336 | pos accuracy:  0.887 | case accuracy:  0.679 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 4000/12416 | loss: 18.6483 | loss pos:  4.4707 | loss case:  3.9314 | loss number:  2.2996 | loss gender:  3.8240 | loss tense:  1.2669 | pos accuracy:  0.947 | case accuracy:  0.895 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 6000/12416 | loss: 18.6531 | loss pos:  4.4953 | loss case:  3.9401 | loss number:  2.3138 | loss gender:  3.8272 | loss tense:  1.2399 | pos accuracy:  1.000 | case accuracy:  1.000 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 8000/12416 | loss: 18.5765 | loss pos:  4.4849 | loss case:  3.9246 | loss number:  2.3029 | loss gender:  3.8228 | loss tense:  1.2177 | pos accuracy:  0.875 | case accuracy:  0.875 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 10000/12416 | loss: 18.5466 | loss pos:  4.4692 | loss case:  3.9215 | loss number:  2.3058 | loss gender:  3.8153 | loss tense:  1.2314 | pos accuracy:  0.944 | case accuracy:  0.889 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 12000/12416 | loss: 18.5281 | loss pos:  4.4665 | loss case:  3.9277 | loss number:  2.3021 | loss gender:  3.8045 | loss tense:  1.2292 | pos accuracy:  0.926 | case accuracy:  0.852 |\n",
      "| epoch: 1/4 | eval loss: 18.54928037467902 | average acc: 0.9377201199531555 | pos acc:  0.912 | case_acc:  0.925\n",
      "__________________________________________________________________________________________\n",
      "| passed: 10000/49449 | loss: 18.1280 | loss pos:  4.3865 | pos_acc:  0.9151 | case_acc:  0.9269 | number_acc:  0.9519 | tense_acc:  0.9772 | pos_acc per last sentence:  0.9333\n",
      "__________________________________________________________________________________________\n",
      "| passed: 20000/49449 | loss: 17.6714 | loss pos:  4.2869 | pos_acc:  0.9175 | case_acc:  0.9291 | number_acc:  0.9527 | tense_acc:  0.9784 | pos_acc per last sentence:  0.9333\n",
      "__________________________________________________________________________________________\n",
      "| passed: 30000/49449 | loss: 17.1803 | loss pos:  4.1640 | pos_acc:  0.9199 | case_acc:  0.9310 | number_acc:  0.9540 | tense_acc:  0.9790 | pos_acc per last sentence:  1.0000\n",
      "__________________________________________________________________________________________\n",
      "| passed: 40000/49449 | loss: 16.8579 | loss pos:  4.0772 | pos_acc:  0.9217 | case_acc:  0.9326 | number_acc:  0.9549 | tense_acc:  0.9795 | pos_acc per last sentence:  1.0000\n",
      "====================================================================================================\n",
      "| epoch: 2/4 | train loss: 16.497842563586644 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 2000/12416 | loss: 15.8862 | loss pos:  3.8010 | loss case:  3.3532 | loss number:  2.0519 | loss gender:  3.2857 | loss tense:  1.0515 | pos accuracy:  0.868 | case accuracy:  0.755 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 4000/12416 | loss: 15.6160 | loss pos:  3.7210 | loss case:  3.2470 | loss number:  2.0464 | loss gender:  3.2019 | loss tense:  1.0972 | pos accuracy:  0.947 | case accuracy:  0.895 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 6000/12416 | loss: 15.6409 | loss pos:  3.7487 | loss case:  3.2472 | loss number:  2.0623 | loss gender:  3.2077 | loss tense:  1.0786 | pos accuracy:  1.000 | case accuracy:  1.000 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 8000/12416 | loss: 15.5524 | loss pos:  3.7351 | loss case:  3.2381 | loss number:  2.0451 | loss gender:  3.1934 | loss tense:  1.0533 | pos accuracy:  0.875 | case accuracy:  1.000 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 10000/12416 | loss: 15.5491 | loss pos:  3.7282 | loss case:  3.2376 | loss number:  2.0475 | loss gender:  3.1948 | loss tense:  1.0672 | pos accuracy:  0.944 | case accuracy:  0.889 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 12000/12416 | loss: 15.5430 | loss pos:  3.7271 | loss case:  3.2495 | loss number:  2.0457 | loss gender:  3.1867 | loss tense:  1.0615 | pos accuracy:  0.926 | case accuracy:  0.889 |\n",
      "| epoch: 2/4 | eval loss: 15.560353757496582 | average acc: 0.9475150108337402 | pos acc:  0.925 | case_acc:  0.937\n",
      "__________________________________________________________________________________________\n",
      "| passed: 10000/49449 | loss: 14.7720 | loss pos:  3.5428 | pos_acc:  0.9319 | case_acc:  0.9406 | number_acc:  0.9598 | tense_acc:  0.9815 | pos_acc per last sentence:  0.8667\n",
      "__________________________________________________________________________________________\n",
      "| passed: 20000/49449 | loss: 14.5706 | loss pos:  3.5081 | pos_acc:  0.9329 | case_acc:  0.9416 | number_acc:  0.9601 | tense_acc:  0.9823 | pos_acc per last sentence:  0.9333\n",
      "__________________________________________________________________________________________\n",
      "| passed: 30000/49449 | loss: 14.3009 | loss pos:  3.4480 | pos_acc:  0.9342 | case_acc:  0.9423 | number_acc:  0.9608 | tense_acc:  0.9826 | pos_acc per last sentence:  1.0000\n",
      "__________________________________________________________________________________________\n",
      "| passed: 40000/49449 | loss: 14.1551 | loss pos:  3.4097 | pos_acc:  0.9350 | case_acc:  0.9430 | number_acc:  0.9613 | tense_acc:  0.9829 | pos_acc per last sentence:  1.0000\n",
      "====================================================================================================\n",
      "| epoch: 3/4 | train loss: 13.93028591660507 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 2000/12416 | loss: 14.9178 | loss pos:  3.6018 | loss case:  3.1772 | loss number:  1.9605 | loss gender:  3.0258 | loss tense:  0.9175 | pos accuracy:  0.868 | case accuracy:  0.792 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 4000/12416 | loss: 14.6299 | loss pos:  3.5277 | loss case:  3.0523 | loss number:  1.9458 | loss gender:  2.9575 | loss tense:  0.9497 | pos accuracy:  0.947 | case accuracy:  0.947 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 6000/12416 | loss: 14.6445 | loss pos:  3.5478 | loss case:  3.0590 | loss number:  1.9565 | loss gender:  2.9581 | loss tense:  0.9402 | pos accuracy:  1.000 | case accuracy:  1.000 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "| passed: 8000/12416 | loss: 14.5552 | loss pos:  3.5271 | loss case:  3.0538 | loss number:  1.9340 | loss gender:  2.9395 | loss tense:  0.9201 | pos accuracy:  1.000 | case accuracy:  0.875 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 10000/12416 | loss: 14.5405 | loss pos:  3.5193 | loss case:  3.0519 | loss number:  1.9325 | loss gender:  2.9412 | loss tense:  0.9301 | pos accuracy:  0.833 | case accuracy:  0.944 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 12000/12416 | loss: 14.5288 | loss pos:  3.5154 | loss case:  3.0625 | loss number:  1.9269 | loss gender:  2.9339 | loss tense:  0.9257 | pos accuracy:  0.926 | case accuracy:  0.889 |\n",
      "| epoch: 3/4 | eval loss: 14.544349717120697 | average acc: 0.9509199261665344 | pos acc:  0.929 | case_acc:  0.941\n",
      "__________________________________________________________________________________________\n",
      "| passed: 10000/49449 | loss: 12.8736 | loss pos:  3.0968 | pos_acc:  0.9407 | case_acc:  0.9473 | number_acc:  0.9638 | tense_acc:  0.9835 | pos_acc per last sentence:  0.9333\n",
      "__________________________________________________________________________________________\n",
      "| passed: 20000/49449 | loss: 12.7492 | loss pos:  3.0778 | pos_acc:  0.9417 | case_acc:  0.9483 | number_acc:  0.9643 | tense_acc:  0.9843 | pos_acc per last sentence:  0.9333\n",
      "__________________________________________________________________________________________\n",
      "| passed: 30000/49449 | loss: 12.5302 | loss pos:  3.0308 | pos_acc:  0.9427 | case_acc:  0.9489 | number_acc:  0.9649 | tense_acc:  0.9849 | pos_acc per last sentence:  1.0000\n",
      "__________________________________________________________________________________________\n",
      "| passed: 40000/49449 | loss: 12.4473 | loss pos:  3.0100 | pos_acc:  0.9431 | case_acc:  0.9494 | number_acc:  0.9652 | tense_acc:  0.9855 | pos_acc per last sentence:  1.0000\n",
      "====================================================================================================\n",
      "| epoch: 4/4 | train loss: 12.281026467853032 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 2000/12416 | loss: 14.5794 | loss pos:  3.5506 | loss case:  3.1341 | loss number:  1.8904 | loss gender:  2.9361 | loss tense:  0.8402 | pos accuracy:  0.868 | case accuracy:  0.755 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 4000/12416 | loss: 14.3142 | loss pos:  3.4905 | loss case:  3.0219 | loss number:  1.8786 | loss gender:  2.8720 | loss tense:  0.8734 | pos accuracy:  0.947 | case accuracy:  0.947 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 6000/12416 | loss: 14.3159 | loss pos:  3.5006 | loss case:  3.0283 | loss number:  1.8920 | loss gender:  2.8673 | loss tense:  0.8639 | pos accuracy:  1.000 | case accuracy:  1.000 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 8000/12416 | loss: 14.2347 | loss pos:  3.4832 | loss case:  3.0249 | loss number:  1.8723 | loss gender:  2.8458 | loss tense:  0.8420 | pos accuracy:  1.000 | case accuracy:  0.875 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 10000/12416 | loss: 14.2015 | loss pos:  3.4747 | loss case:  3.0153 | loss number:  1.8684 | loss gender:  2.8475 | loss tense:  0.8514 | pos accuracy:  0.833 | case accuracy:  0.944 |\n",
      "____________________________________________________________________________________________________\n",
      "| passed: 12000/12416 | loss: 14.1717 | loss pos:  3.4663 | loss case:  3.0194 | loss number:  1.8598 | loss gender:  2.8353 | loss tense:  0.8472 | pos accuracy:  0.963 | case accuracy:  0.926 |\n",
      "| epoch: 4/4 | eval loss: 14.183422394966506 | average acc: 0.9533492922782898 | pos acc:  0.931 | case_acc:  0.942\n"
     ]
    }
   ],
   "source": [
    "lpt, lct, lnt, lgt, ltt, lat, apt, act, ant, agt, att, aat = [], [], [], [], [], [], [], [], [], [], [], []\n",
    "for epoch in range(1, EPOCH + 1):\n",
    "    train_loss, lp, lc, ln, lg, lt, la, ap, ac, an, ag, at, aa = train(model, train_loader, optimizer, criterion)\n",
    "    lpt += lp\n",
    "    lct += lc\n",
    "    lnt += ln\n",
    "    lgt += lg\n",
    "    ltt += lt\n",
    "    lat += la\n",
    "    apt += ap\n",
    "    act += ac\n",
    "    ant += an\n",
    "    agt += ag\n",
    "    att += at\n",
    "    aat += aa\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"| epoch: {epoch}/{EPOCH} | train loss: {train_loss} |\")\n",
    "    eval_loss, acc, pos_acc, case_acc = evaluate(model, test_loader, criterion)\n",
    "    print(f\"| epoch: {epoch}/{EPOCH} | eval loss: {eval_loss} | average acc: {acc} | pos acc: {pos_acc:6.3f} | case_acc: {case_acc:6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e90bbf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ЧИСЛО ЭПОХ СДЕЛАЙ 2 И ПОСТАВЬ ШУДЛЕР БЛЯТЬ НАКОНЕЦ!\n",
    "torch.save(model.state_dict(), \"lstm-cnn-morph-predictor-345d-768h-adam-3e5-epoch-4-update-2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8c59e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 3, 21, 22, 0, 51, 883543)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.utcnow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17e4b313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-03-18 13:11:53.942337'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(datetime(2023, 3, 18, 13, 11, 53, 942337))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6ac809a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_loss</th>\n",
       "      <th>case_loss</th>\n",
       "      <th>number_loss</th>\n",
       "      <th>gender_loss</th>\n",
       "      <th>tense_loss</th>\n",
       "      <th>animacy_loss</th>\n",
       "      <th>pos_acc</th>\n",
       "      <th>case_acc</th>\n",
       "      <th>number_acc</th>\n",
       "      <th>gender_acc</th>\n",
       "      <th>tense_acc</th>\n",
       "      <th>animacy_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53.976917</td>\n",
       "      <td>43.010426</td>\n",
       "      <td>21.555592</td>\n",
       "      <td>29.460543</td>\n",
       "      <td>26.514055</td>\n",
       "      <td>18.933142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45.329891</td>\n",
       "      <td>36.633713</td>\n",
       "      <td>20.301250</td>\n",
       "      <td>27.392374</td>\n",
       "      <td>22.378302</td>\n",
       "      <td>15.239198</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83.570099</td>\n",
       "      <td>66.340607</td>\n",
       "      <td>36.786621</td>\n",
       "      <td>49.101803</td>\n",
       "      <td>40.914780</td>\n",
       "      <td>28.274738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.741935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.222179</td>\n",
       "      <td>12.931485</td>\n",
       "      <td>6.657172</td>\n",
       "      <td>9.612161</td>\n",
       "      <td>8.047033</td>\n",
       "      <td>4.933577</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.487713</td>\n",
       "      <td>2.145606</td>\n",
       "      <td>0.932404</td>\n",
       "      <td>1.696517</td>\n",
       "      <td>1.310476</td>\n",
       "      <td>1.129375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197791</th>\n",
       "      <td>4.637361</td>\n",
       "      <td>9.195228</td>\n",
       "      <td>3.203994</td>\n",
       "      <td>3.361206</td>\n",
       "      <td>0.063446</td>\n",
       "      <td>5.811224</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197792</th>\n",
       "      <td>5.044485</td>\n",
       "      <td>2.067928</td>\n",
       "      <td>1.678415</td>\n",
       "      <td>1.355302</td>\n",
       "      <td>0.626490</td>\n",
       "      <td>3.489312</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197793</th>\n",
       "      <td>9.469631</td>\n",
       "      <td>5.550805</td>\n",
       "      <td>4.595307</td>\n",
       "      <td>3.069457</td>\n",
       "      <td>0.243415</td>\n",
       "      <td>6.804484</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.863636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197794</th>\n",
       "      <td>0.467668</td>\n",
       "      <td>0.627315</td>\n",
       "      <td>0.540148</td>\n",
       "      <td>0.374201</td>\n",
       "      <td>0.181452</td>\n",
       "      <td>0.200805</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197795</th>\n",
       "      <td>3.002784</td>\n",
       "      <td>0.480150</td>\n",
       "      <td>0.769470</td>\n",
       "      <td>1.377182</td>\n",
       "      <td>0.131871</td>\n",
       "      <td>0.885166</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197796 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pos_loss  case_loss  number_loss  gender_loss  tense_loss  \\\n",
       "0       53.976917  43.010426    21.555592    29.460543   26.514055   \n",
       "1       45.329891  36.633713    20.301250    27.392374   22.378302   \n",
       "2       83.570099  66.340607    36.786621    49.101803   40.914780   \n",
       "3       16.222179  12.931485     6.657172     9.612161    8.047033   \n",
       "4        2.487713   2.145606     0.932404     1.696517    1.310476   \n",
       "...           ...        ...          ...          ...         ...   \n",
       "197791   4.637361   9.195228     3.203994     3.361206    0.063446   \n",
       "197792   5.044485   2.067928     1.678415     1.355302    0.626490   \n",
       "197793   9.469631   5.550805     4.595307     3.069457    0.243415   \n",
       "197794   0.467668   0.627315     0.540148     0.374201    0.181452   \n",
       "197795   3.002784   0.480150     0.769470     1.377182    0.131871   \n",
       "\n",
       "        animacy_loss   pos_acc  case_acc  number_acc  gender_acc  tense_acc  \\\n",
       "0          18.933142  0.000000  0.150000    0.450000    0.200000   0.950000   \n",
       "1          15.239198  0.176471  0.000000    0.000000    0.000000   0.882353   \n",
       "2          28.274738  0.000000  0.096774    0.032258    0.032258   0.903226   \n",
       "3           4.933577  0.166667  0.000000    0.333333    0.000000   0.833333   \n",
       "4           1.129375  0.000000  0.000000    1.000000    0.000000   1.000000   \n",
       "...              ...       ...       ...         ...         ...        ...   \n",
       "197791      5.811224  0.875000  0.750000    0.937500    0.875000   1.000000   \n",
       "197792      3.489312  0.933333  0.966667    1.000000    1.000000   1.000000   \n",
       "197793      6.804484  0.863636  0.909091    0.954545    0.954545   1.000000   \n",
       "197794      0.200805  1.000000  1.000000    1.000000    1.000000   1.000000   \n",
       "197795      0.885166  0.954545  1.000000    1.000000    1.000000   1.000000   \n",
       "\n",
       "        animacy_acc  \n",
       "0          0.650000  \n",
       "1          0.823529  \n",
       "2          0.741935  \n",
       "3          1.000000  \n",
       "4          0.000000  \n",
       "...             ...  \n",
       "197791     0.812500  \n",
       "197792     0.933333  \n",
       "197793     0.863636  \n",
       "197794     1.000000  \n",
       "197795     1.000000  \n",
       "\n",
       "[197796 rows x 12 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convertItems(seq: list) -> list:\n",
    "    return [i.item() for i in seq]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"pos_loss\": lpt,\n",
    "    \"case_loss\": lct,\n",
    "    \"number_loss\": lnt,\n",
    "    \"gender_loss\": lgt,\n",
    "    \"tense_loss\": ltt,\n",
    "    \"animacy_loss\": lat,\n",
    "    \"pos_acc\": convertItems(apt),\n",
    "    \"case_acc\": convertItems(act),\n",
    "    \"number_acc\": convertItems(ant),\n",
    "    \"gender_acc\": convertItems(agt),\n",
    "    \"tense_acc\": convertItems(att),\n",
    "    \"animacy_acc\": convertItems(aat)\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72ba3dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"lstm-cnn-result-v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add1f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss, acc, pos_acc, case_acc = evaluate(model, test_loader, criterion)\n",
    "print(f\"| eval loss: {eval_loss:6.3f} | acc: {acc:6.3f} | pos_acc: {pos_acc:6.3f} | case_acc: {case_acc:6.3f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb18d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model.load_state_dict(torch.load(\"lstm-cnn-morph-predictor-345d-768h-adam-3e5-epoch-4-update-2.pt\"))\n",
    "\n",
    "speech_part_unmap = {i: str(s) for i, s in enumerate(PARTS_OF_SPEECH)}\n",
    "speech_part_len = len(PARTS_OF_SPEECH)\n",
    "\n",
    "case_unmap = {i: str(s) for i, s in enumerate(CASES)}\n",
    "case_len = len(CASES)\n",
    "\n",
    "number_unmap = {i: str(s) for i, s in enumerate(NUMBERS)}\n",
    "number_len = len(NUMBERS)\n",
    "\n",
    "gender_unmap = {i: str(s) for i, s in enumerate(GENDERS)}\n",
    "gender_len = len(GENDERS)\n",
    "\n",
    "tense_unmap = {i: str(s) for i, s in enumerate(TENSES)}\n",
    "tense_len = len(TENSES)\n",
    "\n",
    "animacy_unmap = {i: str(s) for i, s in enumerate(ANIMACY)}\n",
    "animacy_len = len(ANIMACY)\n",
    "\n",
    "def clockit(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        func(*args, **kwargs)\n",
    "        print(\"Elapsed time:\", time.time() - start)\n",
    "    return wrapper\n",
    "\n",
    "def sample(model, word, hx=torch.zeros(1, model.hidden), cx=torch.zeros(1, model.hidden)):\n",
    "    m = MorphAnalyzer()\n",
    "    inp = analyze2tensor(m.parse(word)).unsqueeze(0).to(device)\n",
    "    hx, cx, pos, case, number, gender, tense, animacy = model(hx, cx, inp)\n",
    "    pos = speech_part_unmap[torch.softmax(pos.float(), dim=-1).argmax(-1).item()]\n",
    "    case = case_unmap[torch.softmax(case.float(), dim=-1).argmax(-1).item()]\n",
    "    number = number_unmap[torch.softmax(number.float(), dim=-1).argmax(-1).item()]\n",
    "    gender = gender_unmap[torch.softmax(gender.float(), dim=-1).argmax(-1).item()]\n",
    "    tense = tense_unmap[torch.softmax(tense.float(), dim=-1).argmax(-1).item()]\n",
    "    animacy = animacy_unmap[torch.softmax(animacy.float(), dim=-1).argmax(-1).item()]\n",
    "    return hx, cx, pos, case, number, gender, tense, animacy\n",
    "\n",
    "@clockit\n",
    "def parse_sentence(sent: str, model):\n",
    "    sent = sent.split()\n",
    "    hx, cx = torch.zeros(1, model.hidden).to(device), torch.zeros(1, model.hidden).to(device)\n",
    "    for word in sent:\n",
    "        hx, cx, pos, case, number, gender, tense, animacy = sample(model, word, hx, cx)\n",
    "        print(\"Слово:\", word, pos, case, number, gender, tense, animacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01f1eab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слово: веселый ADJ Nom Sing Masc Unk Unk\n",
      "Слово: молочник NOUN Nom Sing Masc Unk Anim\n",
      "Elapsed time: 0.19352126121520996\n"
     ]
    }
   ],
   "source": [
    "parse_sentence(\"веселый молочник\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9db38214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221082 54808\n"
     ]
    }
   ],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "m = MorphAnalyzer()\n",
    "\n",
    "tokens = 0\n",
    "biased = 0\n",
    "\n",
    "with open(\"labeled_syntagrus.test\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file.readlines():\n",
    "        if line.strip() != \"\":\n",
    "            token = line.lower().split(\"\\t\")[1]\n",
    "            if token in punctuation:\n",
    "                tokens += 1\n",
    "                continue\n",
    "            if len(m.parse(token)) > 1 and len(set([x.tag.POS for x in m.parse(token)])) > 1:\n",
    "                biased += 1\n",
    "                tokens += 1\n",
    "            else:\n",
    "                tokens += 1\n",
    "print(tokens, biased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28be20b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
